---
icon: circle-info
bgImage: /assets/images/tonarino.png
---

# Self introduction

## short bio

After I graduated my high school, I majourd in statistics in the School of Economics at Nagoya University.
During the student years, I worked as an intern at [Human Dataware Lab](https://www.hdwlab.co.jp/) and had an part-time job at [Tarvo](https://tarvo.co.jp/).
After I graduted the Nagoya University I joined the IBM Japan, and engaged in the system development for an insurance company.
I expressed my profession in the field of generative AI.

For now, I am a master student at the Language Technologies Institute in School of Computer Science at Carnegie Mellon University.
I joined the WAVLab and mentored by Prof. Shinji Watanabe, and doing my research on speech and language.



## Research Interests

**Speed up inference without losing accuracy**

Recent development of neural network technologies enables us to use the Self-Supervised Learning (SSL) models in the speech field, to improve the accuracy of speech related tasks, such as speech to text.
However, these models usually contains a lot of parameters, slowering the inference speed of the entire speech related systems.
Here I'd like to develop a new technologies to enable these large-parameter models running on a small resource without losing its accuracy and inference speed.


**Generating the structured text directry from speech**

Based on my experience of IBM Japan, I realized that generating structured text from conversation is pretty important.
Especially if we could generate it in a streaming manner, we can expand the technique to our daily business works, such as generating the proceedings or todos, or it can generate a breef summary of the next steps based on the explanation from a senior employee.

I think there are already several services like this by combining the Speech2text and Large Language Models, but I'm interested in whether we can develop a end-to-end streaming model to achieve this.


(Priority is less than above)
**Utilize what we say before**

In concrete, I'm interested in whether machines can transcribe the following video.
This video clip contains Japanese speech from a streaming of Todoroki-hajime.
In my opinion, only talented and trained Japanese, who watched the streaming throughly, can transcribe this speech.
I think machine can achieve this transcription only if it can learn how to utilize what she said before, and the text and images that we can see in the video.

[轟はじめ - 8番出口実況より抜粋した動画](https://www.youtube.com/watch?v=MUTQQj4Q9ug)

